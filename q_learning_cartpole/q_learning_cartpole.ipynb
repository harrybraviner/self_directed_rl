{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(tfk.Model):\n",
    "    \n",
    "    def __init__(self, env, hidden_sizes=[16], act='relu', model_to_copy=None):\n",
    "        super(Agent, self).__init__()\n",
    "        \n",
    "        self.fc_layers = []\n",
    "        for size in hidden_sizes:\n",
    "            self.fc_layers.append(tfk.layers.Dense(size, activation=act))\n",
    "        # Final layer to produce Q(s, :) estimates\n",
    "        self.fc_layers.append(tfk.layers.Dense(env.action_space.n, activation=None))\n",
    "        \n",
    "        # Run this to initialize weights\n",
    "        self(np.zeros(shape=(1,) + env.observation_space.shape, dtype=np.float32))\n",
    "        \n",
    "        if model_to_copy is not None:\n",
    "            for l_this, l_other in zip(self.layers, model_to_copy.layers):\n",
    "                w0 = l_other.get_weights()\n",
    "                l_this.set_weights(w0)\n",
    "    \n",
    "    def soft_update_weights(self, other, gamma=0.75):\n",
    "        for l_this, l_other in zip(self.layers, other.layers):\n",
    "            w_other = l_other.get_weights()\n",
    "            w_this = l_this.get_weights()\n",
    "            # Iterator in here is because w_other and w_this are *lists* of weights at each layer\n",
    "            l_this.set_weights([ww_other*(1.0 - gamma) + gamma*ww_this\n",
    "                                for ww_other, ww_this in zip(w_other, w_this)])\n",
    "    \n",
    "    def call(self, x):\n",
    "        for layer in self.fc_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to be able to copy weights between models - think this may tell me how to do so: https://medium.com/randomai/model-surgery-copy-weights-from-model-to-model-a31b1dec7a7a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(env, agent, explore_eps=0.2, max_steps=200):\n",
    "    \"\"\"\n",
    "    Runs the agent purely in forward mode, and uses its actions to step the env.\n",
    "    \"\"\"\n",
    "\n",
    "    s_hist = []\n",
    "    a_hist = []\n",
    "    r_hist = []\n",
    "    d_hist = []\n",
    "    \n",
    "    qa_max_hist = []\n",
    "    \n",
    "    s = env.reset()\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        \n",
    "        q_estimates = agent(np.array([s], dtype=np.float32))\n",
    "        \n",
    "        a_max = np.argmax(q_estimates, axis=1)[0]\n",
    "        \n",
    "        a_selected = a_max if (np.random.uniform() > explore_eps) \\\n",
    "                           else np.random.choice(env.action_space.n)\n",
    "        \n",
    "        s_next, r, done, _ = env.step(a_max)\n",
    "        \n",
    "        s_hist.append(s)\n",
    "        a_hist.append(a_selected)\n",
    "        r_hist.append(r)\n",
    "        d_hist.append(done)\n",
    "        \n",
    "        qa_max = q_estimates[0, a_max]\n",
    "        qa_max_hist.append(qa_max)\n",
    "        \n",
    "        if done:\n",
    "            s_hist.append(s_next) # Need this in the thing we return\n",
    "            break\n",
    "\n",
    "    ep_reward = sum(r_hist)\n",
    "    \n",
    "    # Package up into (s, a, r, s_next) tuples.\n",
    "    # These are exactly what we need to iteratively impose the Bellman equation.\n",
    "    # We drop the final state - this would be a problem in a game where the reward was all in the final step.\n",
    "    # I don't think that it should be a problem for CartPole.\n",
    "    return list(zip(s_hist[:-1], a_hist, r_hist, s_hist[1:], d_hist)), ep_reward, qa_max_hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(agent, target_agent, optimizer, experiences, gamma=0.99):\n",
    "    \n",
    "    states = np.array([s for s, _, _, _, _ in experiences], dtype=np.float32)\n",
    "    next_states = np.array([ns for _, _, _, ns, _ in experiences], dtype=np.float32)\n",
    "    actions = np.array([a for _, a, _, _, _ in experiences], dtype=np.int64)\n",
    "    rewards = np.array([r for _, _, r, _, _ in experiences], dtype=np.float32)\n",
    "    dones = np.array([d for _, _, _, _, d in experiences], dtype=np.bool)\n",
    "       \n",
    "    # Recall that our agent outputs Q(s, :), with a neuron for each action.\n",
    "    # We must take the max over these to get the RHS of the Bellman equation.\n",
    "    target_qs = tf.reduce_max(target_agent(next_states), axis=1)\n",
    "    targets = rewards + gamma * target_qs * ~dones\n",
    "    \n",
    "    print(list(zip(dones, [t.numpy() for t in targets])))\n",
    "    \n",
    "    # Now we have our targets, we can train our agent towards them\n",
    "    with tf.GradientTape() as tape:\n",
    "        agent_qs_all_a = agent(states)\n",
    "        n = agent_qs_all_a.shape[1]\n",
    "        actions_oh = tf.one_hot(indices=actions, depth=n, on_value=True, off_value=False, dtype=tf.bool)\n",
    "        agent_qs = agent(states)[actions_oh]\n",
    "        loss = tf.losses.mean_squared_error(targets, agent_qs)\n",
    "    gradients = tape.gradient(loss, agent.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, agent.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "agent = Agent(env)\n",
    "\n",
    "target_agent = Agent(env, model_to_copy=agent)\n",
    "optimizer = tf.keras.optimizers.SGD()\n",
    "\n",
    "# Dumb option - just keep all of the experience ever\n",
    "experiences = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes_per_train = 1\n",
    "num_training_steps = 1000\n",
    "\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tracking of performance\n",
    "class EMA:\n",
    "    def __init__(self, gamma=0.99):\n",
    "        self._x = None\n",
    "        self._gamma = gamma\n",
    "    \n",
    "    def update(self, x):\n",
    "        if self._x is None:\n",
    "            self._x = x\n",
    "        else:\n",
    "            self._x *= self._gamma\n",
    "            self._x += (1 - self._gamma) * x\n",
    "    \n",
    "    def get(self):\n",
    "        return self._x\n",
    "    \n",
    "ema_reward = EMA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for step in range(num_training_steps):\n",
    "    # Generate some new episodes\n",
    "    for _ in range(episodes_per_train):\n",
    "        new_experiences, ep_reward, qa_max_hist = generate_episode(env, agent)\n",
    "        \n",
    "        #print([r for _, _, r, _ in new_experiences])\n",
    "        \n",
    "        #print([a for _, a, _, _ in new_experiences])\n",
    "        #print([t.numpy() for t in qa_max_hist])\n",
    "        \n",
    "        experiences += new_experiences\n",
    "        ema_reward.update(ep_reward)\n",
    "    \n",
    "    this_batch_size = min(len(experiences), batch_size)\n",
    "    exp_idx = np.random.choice(len(experiences), replace=False, size=this_batch_size)\n",
    "    experiences_this_batch = [experiences[i] for i in exp_idx]\n",
    "    \n",
    "    train_agent(agent, target_agent, optimizer, experiences_this_batch)\n",
    "    target_agent.soft_update_weights(agent)\n",
    "    \n",
    "    if step % 10 == 0:\n",
    "        print(f'Step: {step}   EMA reward: {ema_reward.get()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
