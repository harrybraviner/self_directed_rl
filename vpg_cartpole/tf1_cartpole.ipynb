{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check versions\n",
    "# I'm requiring python >= 3.6 so I can use f-strings\n",
    "versions_ok = True\n",
    "if tf.__version__.split('.')[0] != '1':\n",
    "    print('Requires tensorflow version 1, but found {}'.format(tf.__version__))\n",
    "    versions_ok = False\n",
    "if sys.version.split('.')[0] != '3' or int([x for s in sys.version.split('.') for x in s.split(' ')][1]) < 6 :\n",
    "    print('Requires python 3.6 or above, but found version {}'.format(sys.version))\n",
    "    versions_ok = False\n",
    "if versions_ok:\n",
    "    print(f'Found versions:\\nTensorflow: {tf.__version__}\\nPython: {sys.version}\\nThese should be fine.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VPGModel:\n",
    "    \n",
    "    def __init__(self, hidden_layer_sizes, env, activation, rng_seed=1234):\n",
    "        # Define the computation graph\n",
    "        self.input_obs = tf.placeholder(shape=(None, env.observation_space.shape[0]), dtype=tf.float32)\n",
    "        \n",
    "        # Save references to these in case we want to inspect them later\n",
    "        self._weights = []\n",
    "        self._biases = []\n",
    "        \n",
    "        x = self.input_obs\n",
    "        prev_layer_size = self.input_obs.shape[1]\n",
    "        for l, this_layer_size in enumerate(hidden_layer_sizes + [env.action_space.n]):\n",
    "            W = tf.get_variable(name=f'layer_{l}_weight', shape=(prev_layer_size, this_layer_size),\n",
    "                               dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.get_variable(name=f'layer_{l}_bias', shape=(this_layer_size),\n",
    "                               dtype=tf.float32, initializer=tf.initializers.constant(0.0))\n",
    "            self._weights.append(W)\n",
    "            self._biases.append(b)\n",
    "\n",
    "            # Replace x with a reference to the output of this layer\n",
    "            x = tf.add(tf.matmul(x, W, name=f'hidden_layer_{l}_matmul'), b, name=f'hidden_layer_{l}_add')\n",
    "            if l < len(hidden_layer_sizes) and activation is not None:\n",
    "                    x = activation(x, name=f'hidden_layer_{l}_activation')\n",
    "\n",
    "            # Update so we get the input size of the next layer correct\n",
    "            prev_layer_size = this_layer_size\n",
    "        \n",
    "        self._action_logits = x\n",
    "        self._action_ps = tf.nn.softmax(self._action_logits)\n",
    "        \n",
    "        self._rng = np.random.RandomState(rng_seed) # Used to sample for the policy.\n",
    "    \n",
    "        # Some additional placeholders that are needed for training, but not for policy sampling.\n",
    "        self.input_actions = tf.placeholder(shape=(None), dtype=tf.int32)\n",
    "        self.input_discounted_rewards = tf.placeholder(shape=(None), dtype=tf.float32)\n",
    "        # This isn't really a loss function. But (for a fixed sample) its derivative is VPG update.\n",
    "        # This allows us to take advantage of TF knowing the derivative of this from the raw logits.\n",
    "        # The reshape is to make clear that the weights should be broadcast in the label (not batch) dimension.\n",
    "        self.pseudo_loss = tf.losses.sparse_softmax_cross_entropy(\n",
    "            labels=self.input_actions,\n",
    "            logits=self._action_logits,\n",
    "            weights=tf.reshape(self.input_discounted_rewards, shape=(-1, 1))\n",
    "        )\n",
    "    \n",
    "    def select_action(self, obs: np.ndarray, sess: tf.Session) -> int:\n",
    "        \"\"\"\n",
    "        Given an observation selects and action from the current policy.\n",
    "        \"\"\"\n",
    "        # Get probability distribution for this action\n",
    "        action_ps = sess.run(self._action_ps, feed_dict={self.input_obs: obs[None, :]})\n",
    "        # Sample from that distribution\n",
    "        return self._rng.choice(action_ps.shape[1], p=action_ps[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueModel:\n",
    "    \"\"\"\n",
    "    Model for the value function of a particular state, i.e. Q(s), under the present policy.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_layer_sizes, env, activation):\n",
    "        # Define our computation graph - similar to VPGModel (except names and output size)\n",
    "        self.input_obs = tf.placeholder(shape=(None, env.observation_space.shape[0]),\n",
    "                                        dtype=tf.float32, name=\"V_input_s\")\n",
    "        \n",
    "        # Save references to these in case we want to inspect them later\n",
    "        self._weights = []\n",
    "        self._biases = []\n",
    "        \n",
    "        x = self.input_obs\n",
    "        prev_layer_size = self.input_obs.shape[1]\n",
    "        for l, this_layer_size in enumerate(hidden_layer_sizes + [1]):\n",
    "            W = tf.get_variable(name=f'V_layer_{l}_weight', shape=(prev_layer_size, this_layer_size),\n",
    "                               dtype=tf.float32, initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.get_variable(name=f'V_layer_{l}_bias', shape=(this_layer_size),\n",
    "                               dtype=tf.float32, initializer=tf.initializers.constant(0.0))\n",
    "            self._weights.append(W)\n",
    "            self._biases.append(b)\n",
    "\n",
    "            # Replace x with a reference to the output of this layer\n",
    "            x = tf.add(tf.matmul(x, W, name=f'V_hidden_layer_{l}_matmul'), b, name=f'V_hidden_layer_{l}_add')\n",
    "            if l < len(hidden_layer_sizes) and activation is not None:\n",
    "                    x = activation(x, name=f'V_hidden_layer_{l}_activation')\n",
    "\n",
    "            # Update so we get the input size of the next layer correct\n",
    "            prev_layer_size = this_layer_size\n",
    "        \n",
    "        self.value_prediction = tf.reshape(x, shape=(-1, ))\n",
    "        \n",
    "        self.input_value_gt = tf.placeholder(shape=[None], dtype=tf.float32, name='V_input_gt')\n",
    "        self.loss = tf.losses.mean_squared_error(labels=self.input_value_gt,\n",
    "                                                 predictions=self.value_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(model, env, sess, max_steps=200):\n",
    "    obs = env.reset()\n",
    "    \n",
    "    observations = []\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        action = model.select_action(obs, sess)\n",
    "        \n",
    "        new_obs, reward, done, _ = env.step(action)\n",
    "        \n",
    "        observations.append(obs)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "        \n",
    "        obs = new_obs\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    return observations, actions, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_batch(model, value_model, env, sess, train_step, train_step_value_model, eps_per_batch=10, gamma=1.0):\n",
    "    \n",
    "    # Check consistency of whether we're using a baseline model or not\n",
    "    if (value_model is None and train_step_value_model is not None)\\\n",
    "        or (value_model is not None and train_step_value_model is None):\n",
    "        raise ValueError('Either none or both of value_model and train_step_value_model should be None')\n",
    "    \n",
    "    observations = []\n",
    "    actions = []\n",
    "    advantages = []\n",
    "    discounted_rewards = []\n",
    "    \n",
    "    ep_total_rewards = []\n",
    "    ep_lengths = [] # Same as total reward for CartPole, but not universally true\n",
    "    \n",
    "    # Assemble samples of trajectories\n",
    "    for ep in range(eps_per_batch):\n",
    "        o_this_ep, a_this_ep, r_this_ep = generate_episode(model, env, sess)\n",
    "        \n",
    "        if value_model is not None:\n",
    "            baseline_values = sess.run(value_model.value_prediction,\n",
    "                                       feed_dict={value_model.input_obs: np.array(o_this_ep)})\n",
    "        else:\n",
    "            baseline_values = np.zeros_like(o_this_ep)\n",
    "        \n",
    "        # Compute the future reward for each step in the episode (possibly with discounting)\n",
    "        disc_r_this_ep = [0.0]*len(r_this_ep)\n",
    "        adv_this_ep = [0.0]*len(r_this_ep)\n",
    "        r_sum = 0\n",
    "        for i in reversed(range(len(r_this_ep))):\n",
    "            r_sum *= gamma\n",
    "            r_sum += r_this_ep[i]\n",
    "            disc_r_this_ep[i] = r_sum\n",
    "            adv_this_ep[i] = disc_r_this_ep[i] - baseline_values[i]\n",
    "        \n",
    "        observations += o_this_ep\n",
    "        actions += a_this_ep\n",
    "        discounted_rewards += disc_r_this_ep\n",
    "        advantages += adv_this_ep\n",
    "\n",
    "        # Will be used to evaluate performance\n",
    "        ep_total_rewards.append(sum(r_this_ep))\n",
    "        ep_lengths.append(len(r_this_ep))\n",
    "    \n",
    "    # Train the model\n",
    "    sess.run(train_step, feed_dict={model.input_obs: np.array(observations),\n",
    "                                    model.input_actions: np.array(actions),\n",
    "                                    model.input_discounted_rewards: np.array(advantages)})\n",
    "    \n",
    "    # If supplied, train the model for the baseline\n",
    "    if value_model is not None:\n",
    "        sess.run(train_step_value_model,\n",
    "                 feed_dict={value_model.input_obs: np.array(observations),\n",
    "                            value_model.input_value_gt: np.array(discounted_rewards)})\n",
    "    \n",
    "    # Return information about this batch\n",
    "    return ep_total_rewards, ep_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_and_report(model, value_model, env, sess, train_step,\n",
    "                            train_step_value_model, num_batches=1000, output_freq=10):\n",
    "    \n",
    "    # Running state, used to index training in different ways\n",
    "    num_episodes = 0\n",
    "    num_steps = 0\n",
    "    start_time_s = time()\n",
    "    \n",
    "    # Statistics collected between outputs, will be periodically reset\n",
    "    total_reward_this_log = 0.0\n",
    "    total_steps_this_log = 0\n",
    "    num_episodes_this_log = 0\n",
    "    min_ep_reward_this_log = None\n",
    "    max_ep_reward_this_log = None\n",
    "    rewards_this_loss = []\n",
    "    \n",
    "    # Logs that we will write values to periodically, to plot later\n",
    "    log_mean_reward_per_ep = []\n",
    "    log_mean_ep_length = []\n",
    "    log_min_reward = []\n",
    "    log_max_reward = []\n",
    "    log_running_num_eps = []\n",
    "    log_running_num_steps = []\n",
    "    log_running_wall_clock_time_s = []\n",
    "    log_10th_percentile = []\n",
    "    log_90th_percentile = []\n",
    "    \n",
    "    for b in range(num_batches):\n",
    "\n",
    "        ep_total_rewards, ep_lengths = train_on_batch(model, value_model, env, sess,\n",
    "                                                      train_step, train_step_value_model)\n",
    "        \n",
    "        # Update state to log\n",
    "        total_reward_this_log += sum(ep_total_rewards)\n",
    "        total_steps_this_log += sum(ep_lengths)\n",
    "        num_episodes_this_log += len(ep_lengths)\n",
    "        if min_ep_reward_this_log is None or min(ep_total_rewards) < min_ep_reward_this_log:\n",
    "            min_ep_reward_this_log = min(ep_total_rewards)\n",
    "        if max_ep_reward_this_log is None or max(ep_total_rewards) > max_ep_reward_this_log:\n",
    "            max_ep_reward_this_log = max(ep_total_rewards)\n",
    "        rewards_this_loss += ep_total_rewards\n",
    "    \n",
    "        if b % output_freq == 0:\n",
    "            # Update the logs\n",
    "            log_mean_reward_per_ep.append(total_reward_this_log / num_episodes_this_log)\n",
    "            log_mean_ep_length.append(total_steps_this_log / num_episodes_this_log)\n",
    "            log_min_reward.append(min_ep_reward_this_log)\n",
    "            log_max_reward.append(max_ep_reward_this_log)\n",
    "            log_running_num_eps.append(num_episodes)\n",
    "            log_running_num_steps.append(num_steps)\n",
    "            log_running_wall_clock_time_s.append(time() - start_time_s)\n",
    "            \n",
    "            rewards_this_loss.sort()\n",
    "            log_10th_percentile.append(rewards_this_loss[(int)(0.1*len(rewards_this_loss))])\n",
    "            log_90th_percentile.append(rewards_this_loss[(int)(0.9*len(rewards_this_loss))])\n",
    "            \n",
    "            # Reset the state\n",
    "            total_reward_this_log = 0.0\n",
    "            total_steps_this_log = 0\n",
    "            num_episodes_this_log = 0\n",
    "            min_ep_reward_this_log = None\n",
    "            max_ep_reward_this_log = None\n",
    "            rewards_this_loss = []\n",
    "\n",
    "            # Print some brief info to the screen\n",
    "            print(f'Trained on {num_episodes} episodes: Mean reward {log_mean_reward_per_ep[-1]}')\n",
    "        \n",
    "        # Why am I updating this *after* updating the logs?\n",
    "        # Because these are used as a count of how many episodes / steps we have trained on.\n",
    "        # And the episodes from this batch had a training step *after* the batch was evaluated.\n",
    "        num_episodes += len(ep_lengths)\n",
    "        num_steps += sum(ep_lengths)\n",
    "    \n",
    "    return log_mean_reward_per_ep, log_mean_ep_length, log_min_reward, log_max_reward, log_running_num_eps,\\\n",
    "        log_running_num_steps, log_running_wall_clock_time_s, log_10th_percentile, log_90th_percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plots(training_results):\n",
    "    log_mean_reward_per_ep, log_mean_ep_length, log_min_reward, log_max_reward, log_running_num_eps,\\\n",
    "        log_running_num_steps, log_running_wall_clock_time_s, log_10th_percentile, log_90th_percentile = training_results\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(7, 4.5))\n",
    "    ax.plot(log_running_num_eps, log_min_reward, '--', color='tab:red', linewidth=1.0)\n",
    "    ax.plot(log_running_num_eps, log_max_reward, '--', color='tab:red', linewidth=1.0)\n",
    "    ax.fill_between(log_running_num_eps, log_10th_percentile, log_90th_percentile, facecolor='tab:red', alpha=0.4)\n",
    "    ax.plot(log_running_num_eps, log_mean_reward_per_ep, '-', color='tab:red')\n",
    "    ax.set_ybound(0, 205)\n",
    "    ax.set_xbound(lower=0.0)\n",
    "    ax.set_xlabel('Num episodes trained on')\n",
    "    ax.set_ylabel('Per-episode total reward')\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(7, 4.5))\n",
    "    ax.plot(log_running_num_steps, log_min_reward, '--', color='tab:red', linewidth=1.0)\n",
    "    ax.plot(log_running_num_steps, log_max_reward, '--', color='tab:red', linewidth=1.0)\n",
    "    ax.fill_between(log_running_num_steps, log_10th_percentile, log_90th_percentile, facecolor='tab:red', alpha=0.4)\n",
    "    ax.plot(log_running_num_steps, log_mean_reward_per_ep, '-', color='tab:red')\n",
    "    ax.set_ybound(0, 205)\n",
    "    ax.set_xbound(lower=0.0)\n",
    "    ax.set_xlabel('Num steps trained on')\n",
    "    ax.set_ylabel('Per-episode total reward')\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(7, 4.5))\n",
    "    ax.plot(np.array(log_running_wall_clock_time_s) / 60.0, log_min_reward, '--', color='tab:red', linewidth=1.0)\n",
    "    ax.plot(np.array(log_running_wall_clock_time_s) / 60.0, log_max_reward, '--', color='tab:red', linewidth=1.0)\n",
    "    ax.fill_between(np.array(log_running_wall_clock_time_s) / 60.0, log_10th_percentile, log_90th_percentile, facecolor='tab:red', alpha=0.4)\n",
    "    ax.plot(np.array(log_running_wall_clock_time_s) / 60.0, log_mean_reward_per_ep, '-', color='tab:red')\n",
    "    ax.set_ybound(0, 205)\n",
    "    ax.set_xbound(lower=0.0)\n",
    "    ax.set_xlabel('Training time [minutes]')\n",
    "    ax.set_ylabel('Per-episode total reward')\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(7, 4.5))\n",
    "    ax.plot(np.array(log_running_wall_clock_time_s) / 60.0, log_running_num_eps, '-', color='tab:blue')\n",
    "    ax.set_ybound(lower=0)\n",
    "    ax.set_xbound(lower=0.0)\n",
    "    ax.set_xlabel('Training time [minutes]')\n",
    "    ax.set_ylabel('Num episodes', color='tab:blue')\n",
    "    ax.tick_params(axis='y', labelcolor='tab:blue')\n",
    "    \n",
    "    ax2 = ax.twinx()\n",
    "    ax2.plot(np.array(log_running_wall_clock_time_s) / 60.0, log_running_num_steps, '--', color='tab:red')\n",
    "    ax2.set_ybound(lower=0)\n",
    "    ax2.set_ylabel('Num steps', color='tab:red')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our model, environment, session, and optimizer\n",
    "env = gym.make('CartPole-v0')\n",
    "tf.reset_default_graph() # Necessary because I'm naming the layers explicitly\n",
    "\n",
    "model = VPGModel(hidden_layer_sizes=[16], env=env, activation=tf.nn.relu)\n",
    "train_step = tf.train.GradientDescentOptimizer(learning_rate=1e-2).minimize(model.pseudo_loss)\n",
    "\n",
    "value_model = ValueModel(hidden_layer_sizes=[16], env=env, activation=tf.nn.relu)\n",
    "train_step_value_model = tf.train.GradientDescentOptimizer(learning_rate=1e-2).minimize(value_model.loss)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Actually run the training, and save the results so we can plots them\n",
    "training_results = run_training_and_report(model, value_model, env, sess, train_step,\n",
    "                                           train_step_value_model, num_batches=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_plots(training_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
