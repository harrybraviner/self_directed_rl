{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "from time import time\n",
    "from math import floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_hidden_units = 32\n",
    "num_episodes_per_training = 200\n",
    "num_episodes_to_train_on = 10000\n",
    "log_freq=200\n",
    "\n",
    "discount_gamma = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global RNG, can replace later\n",
    "rng = np.random.RandomState(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(Model):\n",
    "    def __init__(self, num_hidden_units=8):\n",
    "        super(Agent, self).__init__()\n",
    "        self.d1 = Dense(units=num_hidden_units, activation='tanh')\n",
    "        self.d2 = Dense(units=num_hidden_units, activation='relu')\n",
    "        self.d3 = Dense(units=1, activation=None)\n",
    "        \n",
    "    def call(self, x):\n",
    "        x = self.d1(x)\n",
    "        #x = self.d2(x)\n",
    "        return self.d3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(agent, env, max_steps=100):\n",
    "    s = env.reset()\n",
    "    \n",
    "    ep_states = []\n",
    "    ep_actions = []\n",
    "    ep_rewards = []\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # Generate an action using the current policy\n",
    "        a_logit = agent(s.reshape((1, -1)))[0]\n",
    "        # Turn the distribution into an action stochastically\n",
    "        a = 1 if sigmoid(a_logit) > rng.uniform(low=0.0, high=1.0) else 0\n",
    "\n",
    "        s1, r, done, _ = env.step(a)\n",
    "        \n",
    "        # Save more of the episode history\n",
    "        ep_states.append(s)\n",
    "        ep_actions.append(a)\n",
    "        ep_rewards.append(r)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "        s = s1\n",
    "\n",
    "    return ep_states, ep_actions, ep_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "agent = Agent(num_hidden_units=num_hidden_units)\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.Adam(learning_rate=1e-2)\n",
    "\n",
    "@tf.function\n",
    "def train_step(states, actions, rewards):\n",
    "    actions = tf.reshape(actions, shape=(-1, 1))\n",
    "    rewards = tf.reshape(rewards, shape=(-1, 1))\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(agent.trainable_variables)\n",
    "        a_logits = agent(states)\n",
    "        loss = loss_object(y_true=actions, y_pred=a_logits, sample_weight=rewards)\n",
    "        #loss = rewards * (actions * tf.math.log(1.0 + tf.math.exp(-a_logits)) + (1.0 - actions) * tf.math.log(1.0 + tf.math.exp(a_logits)))\n",
    "    gradients = tape.gradient(loss, agent.trainable_variables)\n",
    "#     tf.print(gradients)\n",
    "#     tf.print(\"------ -------- ------\")\n",
    "#     tf.print(a_logits)\n",
    "#     tf.print(\"******** ************* *********\")\n",
    "    optimizer.apply_gradients(zip(gradients, agent.trainable_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer_states = []\n",
    "buffer_actions = []\n",
    "buffer_discounted_rewards = []\n",
    "\n",
    "summed_ep_rewards = 0.0\n",
    "\n",
    "start_time_s = time()\n",
    "\n",
    "for episode in range(num_episodes_to_train_on):\n",
    "\n",
    "    # Generate an episode using the current policy\n",
    "    ep_states, ep_actions, ep_rewards = generate_episode(agent, env)\n",
    "    \n",
    "    # Update our metrics of performance\n",
    "    total_reward = sum(ep_rewards)\n",
    "    summed_ep_rewards += total_reward\n",
    "    \n",
    "    # Compute the discounted rewards\n",
    "    ep_discounted_rewards = np.zeros(shape=len(ep_rewards))\n",
    "    running_reward = 0.0\n",
    "    for i in range(len(ep_rewards)-1, -1, -1):\n",
    "        running_reward *= discount_gamma\n",
    "        running_reward += ep_rewards[i]\n",
    "        ep_discounted_rewards[i] = running_reward\n",
    "    \n",
    "    # Add to the buffers\n",
    "    buffer_states.append(ep_states)\n",
    "    buffer_actions.append(ep_actions)\n",
    "    buffer_discounted_rewards.append(ep_discounted_rewards)\n",
    "    \n",
    "    if (episode + 1) % num_episodes_per_training == 0:\n",
    "        buffer_states = np.concatenate(buffer_states).astype(np.float32)\n",
    "        buffer_actions = np.concatenate(buffer_actions).astype(np.float32)\n",
    "        buffer_discounted_rewards = np.concatenate(buffer_discounted_rewards).astype(np.float32)\n",
    "        \n",
    "        # Shuffle rows (only really need this if we're going to batch!)\n",
    "#         indices = list(range(buffer_states.shape[0]))\n",
    "#         rng.shuffle(indices)\n",
    "#         buffer_states = buffer_states[indices]\n",
    "#         buffer_actions = buffer_actions[indices]\n",
    "#         buffer_discounted_rewards = buffer_discounted_rewards[indices]\n",
    "\n",
    "        train_step(buffer_states, buffer_actions, buffer_discounted_rewards)\n",
    "        \n",
    "        # Clear the buffers\n",
    "        buffer_states = []\n",
    "        buffer_actions = []\n",
    "        buffer_discounted_rewards = []\n",
    "\n",
    "    if (episode + 1) % log_freq == 0:\n",
    "        per_ep_reward = summed_ep_rewards / log_freq\n",
    "        summed_ep_rewards = 0.0\n",
    "        \n",
    "        time_now_s = time()\n",
    "        elapsed_time_s = time_now_s - start_time_s\n",
    "        elapsed_part_m = floor(elapsed_time_s / 60)\n",
    "        elapsed_part_s = elapsed_time_s - 60 * elapsed_part_m\n",
    "        \n",
    "        print(\"[{}m {:0.2f}s] episodes: {}\\tMean reward: {}\".format(elapsed_part_m, elapsed_part_s,\n",
    "                                                                    episode + 1, per_ep_reward))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
